{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AdaMatch.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyOrLAqaE8ou2iVdBAbj3lo4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qyouE3TtcKMM","executionInfo":{"status":"ok","timestamp":1641393526966,"user_tz":-540,"elapsed":18231,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}},"outputId":"911b50ef-1a72-48b3-92cb-24186419fd4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 1.8 MB 9.9 MB/s \n","\u001b[K     |████████████████████████████████| 4.9 MB 47.9 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n","\u001b[K     |████████████████████████████████| 47.7 MB 76.9 MB/s \n","\u001b[K     |████████████████████████████████| 213 kB 52.5 MB/s \n","\u001b[K     |████████████████████████████████| 99 kB 9.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.2 MB 52.9 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 8.1 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 57.9 MB/s \n","\u001b[K     |████████████████████████████████| 352 kB 56.8 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 46.9 MB/s \n","\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q tf-models-official"]},{"cell_type":"code","source":["import tensorflow as tf\n","tf.random.set_seed(42)\n","\n","import numpy as np\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","from official.vision.image_classification.augment import RandAugment\n","\n","import tensorflow_datasets as tfds\n","\n","tfds.disable_progress_bar()"],"metadata":{"id":"jO6HjL_ccQCt","executionInfo":{"status":"ok","timestamp":1641393529364,"user_tz":-540,"elapsed":2400,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Data preparation"],"metadata":{"id":"YQuJ6yMZdP4s"}},{"cell_type":"code","source":["# MNIST\n","(\n","    (mnist_x_train, mnist_y_train),\n","    (mnist_x_test, mnist_y_test),\n",") = keras.datasets.mnist.load_data()\n","\n","#Add a channel dimension\n","mnist_x_train = tf.expand_dims(mnist_x_train, -1)\n","mnist_x_test = tf.expand_dims(mnist_x_test, -1)\n","\n","#Convert the labels to one-hot encoded vectors\n","mnist_y_train = tf.one_hot(mnist_y_train, 10).numpy()\n","\n","#SVHN\n","svhn_train, svhn_test = tfds.load(\n","    \"svhn_cropped\", split=[\"train\", \"test\"], as_supervised=True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qga9kKyVcrDw","executionInfo":{"status":"ok","timestamp":1641394296859,"user_tz":-540,"elapsed":767500,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}},"outputId":"b675ea8f-b747-4e58-b320-6b0874c1106e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n","\u001b[1mDownloading and preparing dataset svhn_cropped/3.0.0 (download: 1.47 GiB, generated: Unknown size, total: 1.47 GiB) to /root/tensorflow_datasets/svhn_cropped/3.0.0...\u001b[0m\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteJUE5CR/svhn_cropped-train.tfrecord\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteJUE5CR/svhn_cropped-test.tfrecord\n","Shuffling and writing examples to /root/tensorflow_datasets/svhn_cropped/3.0.0.incompleteJUE5CR/svhn_cropped-extra.tfrecord\n","\u001b[1mDataset svhn_cropped downloaded and prepared to /root/tensorflow_datasets/svhn_cropped/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"]}]},{"cell_type":"markdown","source":["# const and hyperparameter"],"metadata":{"id":"NMyrbtSqc12P"}},{"cell_type":"code","source":["RESIZE_TO = 32\n","\n","SOURCE_BATCH_SIZE = 64\n","TARGET_BATCH_SIZE = 3 * SOURCE_BATCH_SIZE #Reference: Section 3.2\n","EPOCHS = 10\n","STEPS_PER_EPOCH = len(mnist_x_train) // SOURCE_BATCH_SIZE\n","TOTAL_STEPS = EPOCHS * STEPS_PER_EPOCH\n","\n","AUTO = tf.data.AUTOTUNE\n","LEARNING_RATE = 0.03\n","WEIGHT_DECAY = 0.0005\n","INIT = \"he_normal\"\n","DEPTH = 28\n","WIDTH_MULT = 2"],"metadata":{"id":"edGyauP5dVsO","executionInfo":{"status":"ok","timestamp":1641394296860,"user_tz":-540,"elapsed":10,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# DATA AUGMENTATION"],"metadata":{"id":"CT_okeSGdyYw"}},{"cell_type":"code","source":["# Initialize `RandAugment` object with 2 layers of\n","# augmentation transforms and strength of 5.\n","augmenter = RandAugment(num_layers=2, magnitude=5)\n","\n","\n","def weak_augment(image, source=True):\n","    if image.dtype != tf.float32:\n","        image = tf.cast(image, tf.float32)\n","\n","    # MNIST images are grayscale, this is why we first convert them to\n","    # RGB images.\n","    if source:\n","        image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n","        image = tf.tile(image, [1, 1, 3])\n","    image = tf.image.random_flip_left_right(image)\n","    image = tf.image.random_crop(image, (RESIZE_TO, RESIZE_TO, 3))\n","    return image\n","\n","\n","def strong_augment(image, source=True):\n","    if image.dtype != tf.float32:\n","        image = tf.cast(image, tf.float32)\n","\n","    if source:\n","        image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n","        image = tf.tile(image, [1, 1, 3])\n","    image = augmenter.distort(image)\n","    return image"],"metadata":{"id":"kbXc4_OfdxlM","executionInfo":{"status":"ok","timestamp":1641395087831,"user_tz":-540,"elapsed":566,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# DATA LOADING"],"metadata":{"id":"rj6T7PahhKKg"}},{"cell_type":"code","source":["def create_individual_ds(ds, aug_func, source=True):\n","    if source:\n","        batch_size = SOURCE_BATCH_SIZE\n","    else:\n","        # During training 3x more target unlabeled samples are shown\n","        # to the model in AdaMatch (Section 3.2 of the paper).\n","        batch_size = TARGET_BATCH_SIZE\n","    ds = ds.shuffle(batch_size * 10, seed=42)\n","\n","    if source:\n","        ds = ds.map(lambda x, y: (aug_func(x), y), num_parallel_calls=AUTO)\n","    else:\n","        ds = ds.map(lambda x, y: (aug_func(x, False), y), num_parallel_calls=AUTO)\n","\n","    ds = ds.batch(batch_size).prefetch(AUTO)\n","    return ds"],"metadata":{"id":"zXknfDqHhFId","executionInfo":{"status":"ok","timestamp":1641395094281,"user_tz":-540,"elapsed":352,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["source_ds = tf.data.Dataset.from_tensor_slices((mnist_x_train, mnist_y_train))\n","source_ds_w = create_individual_ds(source_ds, weak_augment)\n","source_ds_s = create_individual_ds(source_ds, strong_augment)\n","final_source_ds = tf.data.Dataset.zip((source_ds_w, source_ds_s))\n","\n","target_ds_w = create_individual_ds(svhn_train, weak_augment, source=False)\n","target_ds_s = create_individual_ds(svhn_train, strong_augment, source=False)\n","final_target_ds = tf.data.Dataset.zip((target_ds_w, target_ds_s))"],"metadata":{"id":"TUoupamyhsin","executionInfo":{"status":"ok","timestamp":1641395106466,"user_tz":-540,"elapsed":6098,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":["# 손실 계산 유틸리티"],"metadata":{"id":"jljOt9-NimKH"}},{"cell_type":"code","source":["def compute_loss_source(source_labels, logits_source_w, logits_source_s):\n","    loss_func = keras.losses.CategoricalCrossentropy(from_logits=True)\n","    # First compute the losses between original source labels and\n","    # predictions made on the weakly and strongly augmented versions\n","    # of the same images.\n","    w_loss = loss_func(source_labels, logits_source_w)\n","    s_loss = loss_func(source_labels, logits_source_s)\n","    return w_loss + s_loss\n","\n","\n","def compute_loss_target(target_pseudo_labels_w, logits_target_s, mask):\n","    loss_func = keras.losses.CategoricalCrossentropy(from_logits=True, reduction=\"none\")\n","    target_pseudo_labels_w = tf.stop_gradient(target_pseudo_labels_w)\n","    # For calculating loss for the target samples, we treat the pseudo labels\n","    # as the ground-truth. These are not considered during backpropagation\n","    # which is a standard SSL practice.\n","    target_loss = loss_func(target_pseudo_labels_w, logits_target_s)\n","\n","    # More on `mask` later.\n","    mask = tf.cast(mask, target_loss.dtype)\n","    target_loss *= mask\n","    return tf.reduce_mean(target_loss, 0)"],"metadata":{"id":"9potiFGJiUM1","executionInfo":{"status":"ok","timestamp":1641395110286,"user_tz":-540,"elapsed":610,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class AdaMatch(keras.Model):\n","    def __init__(self, model, total_steps, tau=0.9):\n","        super(AdaMatch, self).__init__()\n","        self.model = model\n","        self.tau = tau  # Denotes the confidence threshold\n","        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n","        self.total_steps = total_steps\n","        self.current_step = tf.Variable(0, dtype=\"int64\")\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker]\n","\n","    # This is a warmup schedule to update the weight of the\n","    # loss contributed by the target unlabeled samples. More\n","    # on this in the text.\n","    def compute_mu(self):\n","        pi = tf.constant(np.pi, dtype=\"float32\")\n","        step = tf.cast(self.current_step, dtype=\"float32\")\n","        return 0.5 - tf.cos(tf.math.minimum(pi, (2 * pi * step) / self.total_steps)) / 2\n","\n","    def train_step(self, data):\n","        ## Unpack and organize the data ##\n","        source_ds, target_ds = data\n","        (source_w, source_labels), (source_s, _) = source_ds\n","        (\n","            (target_w, _),\n","            (target_s, _),\n","        ) = target_ds  # Notice that we are NOT using any labels here.\n","\n","        combined_images = tf.concat([source_w, source_s, target_w, target_s], 0)\n","        combined_source = tf.concat([source_w, source_s], 0)\n","\n","        total_source = tf.shape(combined_source)[0]\n","        total_target = tf.shape(tf.concat([target_w, target_s], 0))[0]\n","\n","        with tf.GradientTape() as tape:\n","            ## Forward passes ##\n","            combined_logits = self.model(combined_images, training=True)\n","            z_d_prime_source = self.model(\n","                combined_source, training=False\n","            )  # No BatchNorm update.\n","            z_prime_source = combined_logits[:total_source]\n","\n","            ## 1. Random logit interpolation for the source images ##\n","            lambd = tf.random.uniform((total_source, 10), 0, 1)\n","            final_source_logits = (lambd * z_prime_source) + (\n","                (1 - lambd) * z_d_prime_source\n","            )\n","\n","            ## 2. Distribution alignment (only consider weakly augmented images) ##\n","            # Compute softmax for logits of the WEAKLY augmented SOURCE images.\n","            y_hat_source_w = tf.nn.softmax(final_source_logits[: tf.shape(source_w)[0]])\n","\n","            # Extract logits for the WEAKLY augmented TARGET images and compute softmax.\n","            logits_target = combined_logits[total_source:]\n","            logits_target_w = logits_target[: tf.shape(target_w)[0]]\n","            y_hat_target_w = tf.nn.softmax(logits_target_w)\n","\n","            # Align the target label distribution to that of the source.\n","            expectation_ratio = tf.reduce_mean(y_hat_source_w) / tf.reduce_mean(\n","                y_hat_target_w\n","            )\n","            y_tilde_target_w = tf.math.l2_normalize(\n","                y_hat_target_w * expectation_ratio, 1\n","            )\n","\n","            ## 3. Relative confidence thresholding ##\n","            row_wise_max = tf.reduce_max(y_hat_source_w, axis=-1)\n","            final_sum = tf.reduce_mean(row_wise_max, 0)\n","            c_tau = self.tau * final_sum\n","            mask = tf.reduce_max(y_tilde_target_w, axis=-1) >= c_tau\n","\n","            ## Compute losses (pay attention to the indexing) ##\n","            source_loss = compute_loss_source(\n","                source_labels,\n","                final_source_logits[: tf.shape(source_w)[0]],\n","                final_source_logits[tf.shape(source_w)[0] :],\n","            )\n","            target_loss = compute_loss_target(\n","                y_tilde_target_w, logits_target[tf.shape(target_w)[0] :], mask\n","            )\n","\n","            t = self.compute_mu()  # Compute weight for the target loss\n","            total_loss = source_loss + (t * target_loss)\n","            self.current_step.assign_add(\n","                1\n","            )  # Update current training step for the scheduler\n","\n","        gradients = tape.gradient(total_loss, self.model.trainable_variables)\n","        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n","\n","        self.loss_tracker.update_state(total_loss)\n","        return {\"loss\": self.loss_tracker.result()}"],"metadata":{"id":"JUMVhFtviYj8","executionInfo":{"status":"ok","timestamp":1641395110741,"user_tz":-540,"elapsed":1,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# Wide-ResNet-28-2 instancification"],"metadata":{"id":"9GXu_EHmn2zB"}},{"cell_type":"code","source":["def wide_basic(x, n_input_plane, n_output_plane, stride):\n","    conv_params = [[3, 3, stride, \"same\"], [3, 3, (1, 1), \"same\"]]\n","\n","    n_bottleneck_plane = n_output_plane\n","\n","    # Residual block\n","    for i, v in enumerate(conv_params):\n","        if i == 0:\n","            if n_input_plane != n_output_plane:\n","                x = layers.BatchNormalization()(x)\n","                x = layers.Activation(\"relu\")(x)\n","                convs = x\n","            else:\n","                convs = layers.BatchNormalization()(x)\n","                convs = layers.Activation(\"relu\")(convs)\n","            convs = layers.Conv2D(\n","                n_bottleneck_plane,\n","                (v[0], v[1]),\n","                strides=v[2],\n","                padding=v[3],\n","                kernel_initializer=INIT,\n","                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","                use_bias=False,\n","            )(convs)\n","        else:\n","            convs = layers.BatchNormalization()(convs)\n","            convs = layers.Activation(\"relu\")(convs)\n","            convs = layers.Conv2D(\n","                n_bottleneck_plane,\n","                (v[0], v[1]),\n","                strides=v[2],\n","                padding=v[3],\n","                kernel_initializer=INIT,\n","                kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","                use_bias=False,\n","            )(convs)\n","\n","    # Shortcut connection: identity function or 1x1\n","    # convolutional\n","    #  (depends on difference between input & output shape - this\n","    #   corresponds to whether we are using the first block in\n","    #   each\n","    #   group; see `block_series()`).\n","    if n_input_plane != n_output_plane:\n","        shortcut = layers.Conv2D(\n","            n_output_plane,\n","            (1, 1),\n","            strides=stride,\n","            padding=\"same\",\n","            kernel_initializer=INIT,\n","            kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","            use_bias=False,\n","        )(x)\n","    else:\n","        shortcut = x\n","\n","    return layers.Add()([convs, shortcut])\n","\n","# Stacking residual units on the same stage\n","def block_series(x, n_input_plane, n_output_plane, count, stride):\n","    x = wide_basic(x, n_input_plane, n_output_plane, stride)\n","    for i in range(2, int(count + 1)):\n","        x = wide_basic(x, n_output_plane, n_output_plane, stride=1)\n","    return x"],"metadata":{"id":"rYyrKN2TkRKB","executionInfo":{"status":"ok","timestamp":1641395111166,"user_tz":-540,"elapsed":3,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["def get_network(image_size=32, num_classes=10):\n","    n = (DEPTH - 4) / 6\n","    n_stages = [16, 16 * WIDTH_MULT, 32 * WIDTH_MULT, 64 * WIDTH_MULT]\n","\n","    inputs = keras.Input(shape=(image_size, image_size, 3))\n","    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n","\n","    conv1 = layers.Conv2D(\n","        n_stages[0],\n","        (3, 3),\n","        strides=1,\n","        padding=\"same\",\n","        kernel_initializer=INIT,\n","        kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","        use_bias=False,\n","    )(x)\n","\n","    ## Add wide residual blocks ##\n","\n","    conv2 = block_series(\n","        conv1,\n","        n_input_plane=n_stages[0],\n","        n_output_plane=n_stages[1],\n","        count=n,\n","        stride=(1, 1),\n","    )  # Stage 1\n","\n","    conv3 = block_series(\n","        conv2,\n","        n_input_plane=n_stages[1],\n","        n_output_plane=n_stages[2],\n","        count=n,\n","        stride=(2, 2),\n","    )  # Stage 2\n","\n","    conv4 = block_series(\n","        conv3,\n","        n_input_plane=n_stages[2],\n","        n_output_plane=n_stages[3],\n","        count=n,\n","        stride=(2, 2),\n","    )  # Stage 3\n","\n","    batch_norm = layers.BatchNormalization()(conv4)\n","    relu = layers.Activation(\"relu\")(batch_norm)\n","\n","    # Classifier\n","    trunk_outputs = layers.GlobalAveragePooling2D()(relu)\n","    outputs = layers.Dense(\n","        num_classes, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n","    )(trunk_outputs)\n","\n","    return keras.Model(inputs, outputs)"],"metadata":{"id":"_9WO3d-rqI2U","executionInfo":{"status":"ok","timestamp":1641395111166,"user_tz":-540,"elapsed":3,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["wrn_model = get_network()\n","print(f\"Model has {wrn_model.count_params()/1e6} Million parameters.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nVmywwoKq3fT","executionInfo":{"status":"ok","timestamp":1641395112248,"user_tz":-540,"elapsed":513,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}},"outputId":"a370b9b3-4ae7-4c9d-c498-ec52808e42d2"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Model has 1.471226 Million parameters.\n"]}]},{"cell_type":"code","source":["reduce_lr = keras.optimizers.schedules.CosineDecay(LEARNING_RATE, TOTAL_STEPS, 0.25)\n","optimizer = keras.optimizers.Adam(reduce_lr)\n","\n","adamatch_trainer = AdaMatch(model=wrn_model, total_steps=TOTAL_STEPS)\n","adamatch_trainer.compile(optimizer=optimizer)"],"metadata":{"id":"eT-SlnsNrAcw","executionInfo":{"status":"ok","timestamp":1641395112249,"user_tz":-540,"elapsed":2,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["total_ds = tf.data.Dataset.zip((final_source_ds, final_target_ds))\n","adamatch_trainer.fit(total_ds, epochs=EPOCHS)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jzHgghiNw8rp","executionInfo":{"status":"ok","timestamp":1641395851219,"user_tz":-540,"elapsed":737040,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}},"outputId":"04bbfa28-97c8-4491-c41f-e676dae48787"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","382/382 [==============================] - 73s 176ms/step - loss: 17140712448.0000\n","Epoch 2/10\n","382/382 [==============================] - 68s 176ms/step - loss: 1.8023\n","Epoch 3/10\n","382/382 [==============================] - 68s 176ms/step - loss: 1.3141\n","Epoch 4/10\n","382/382 [==============================] - 68s 176ms/step - loss: 2.2112\n","Epoch 5/10\n","382/382 [==============================] - 68s 176ms/step - loss: 6.3335\n","Epoch 6/10\n","382/382 [==============================] - 68s 176ms/step - loss: 11.0698\n","Epoch 7/10\n","382/382 [==============================] - 68s 176ms/step - loss: 14.1695\n","Epoch 8/10\n","382/382 [==============================] - 68s 176ms/step - loss: 14.8883\n","Epoch 9/10\n","382/382 [==============================] - 68s 177ms/step - loss: 15.7434\n","Epoch 10/10\n","382/382 [==============================] - 68s 178ms/step - loss: 18.2552\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7ff697e79190>"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# Compile the AdaMatch model to yield accuracy.\n","adamatch_trained_model = adamatch_trainer.model\n","adamatch_trained_model.compile(metrics=keras.metrics.SparseCategoricalAccuracy())\n","\n","# Score on the target test set.\n","svhn_test = svhn_test.batch(TARGET_BATCH_SIZE).prefetch(AUTO)\n","_, accuracy = adamatch_trained_model.evaluate(svhn_test)\n","print(f\"Accuracy on target test set: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y9qtElHbrbtX","executionInfo":{"status":"ok","timestamp":1641395854834,"user_tz":-540,"elapsed":3618,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}},"outputId":"5e2209e3-c377-43f7-d3ab-8bd8542d5e44"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["136/136 [==============================] - 4s 22ms/step - loss: 683.9866 - sparse_categorical_accuracy: 0.0862\n","Accuracy on target test set: 8.62%\n"]}]},{"cell_type":"code","source":["# Utility function for preprocessing the source test set.\n","def prepare_test_ds_source(image, label):\n","    image = tf.image.resize_with_pad(image, RESIZE_TO, RESIZE_TO)\n","    image = tf.tile(image, [1, 1, 3])\n","    return image, label\n","\n","\n","source_test_ds = tf.data.Dataset.from_tensor_slices((mnist_x_test, mnist_y_test))\n","source_test_ds = (\n","    source_test_ds.map(prepare_test_ds_source, num_parallel_calls=AUTO)\n","    .batch(TARGET_BATCH_SIZE)\n","    .prefetch(AUTO)\n",")\n","\n","# Evaluation on the source test set.\n","_, accuracy = adamatch_trained_model.evaluate(source_test_ds)\n","print(f\"Accuracy on source test set: {accuracy * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aFmR7yn2r2ap","executionInfo":{"status":"ok","timestamp":1641395857885,"user_tz":-540,"elapsed":1828,"user":{"displayName":"이서영","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08706917529998634867"}},"outputId":"c46eea95-74b3-4bca-ba04-ae66757a2e5f"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["53/53 [==============================] - 2s 19ms/step - loss: 683.9869 - sparse_categorical_accuracy: 0.9452\n","Accuracy on source test set: 94.52%\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"8EBQsoBbsCjN"},"execution_count":null,"outputs":[]}]}